# General note:

* Set bridge adapter before setup & enable 'Allow all'.
* Setup proxy & edit wired connection, under 'IPv4 Settings' tab, as follows:

Netmask : 255.255.128.0
Gateway : 10.6.0.1
DNS     : 10.101.1.10

--------------------------------------------------------------
# Ex 3: Java installation

sudo apt-get install default-jdk
java -version

---------------------
* Sample java code

import java.io.*;

public class helloWorld {
	public static void main(String[] args) {
		System.out.println("Hello world!");
	}
}

---------------------

javac helloWorld.java
java helloWorld

--------------------------------------------------------------
# Ex 4: Remote login

sudo apt-get install openssh-server
ssh vm2@ip_of_vm2

ssh-keygen -t rsa
cd ~/.ssh
chmod 700 id_rsa.pub
cp id_rsa.pub known_hosts
ssh-copy-id vm2@ip_of_vm2
ssh vm2@ip_of_vm2

--------------------------------------------------------------
# Ex 5: File transfer

scp /home/vm1/Desktop/file.txt vm2@ip_of_vm2:/home/vm2/Desktop/

--------------------------------------------------------------
# Ex 6: Eucalyptus

* Install cloud (cc) & node (nc) controller.
* Create another VM for client (cl).

---------------------
* Follow the below steps in (cl).

* Configure 'No proxy' & type below url in browser.
https://ip_of_cc:8443

* Use UN & PW as 'admin' to sign-in.
* Download the credentials under 'credentials' tab.

scp file.zip cc@ip_of_cc:/home/cc

---------------------
* Follow the below steps in (cc)

mkdir -p ~/.euca
cp file.zip .euca
cd ~/.euca
unzip file.zip

---------------------
* Follow the below steps in (cl)

sudo apt-get update
sudo apt-get install euca2ools
unzip file.zip

* Open the eucarc file and note down EC2_URL, EC2_accessKey and EC2_secretkey

euca-create-volume -U ec2_URL -I ec2_accessKey -S ec2_secretKey --size 1 -z clustername
euca-describe-volumes -U ec2_URL -I ec2_accessKey -S ec2_secretKey

--------------------------------------------------------------
# Ex 7: Opennebula

* Switch to Main server in 'Software & Updates' settings.

---------------------
* Front-end installation (vm1)

wget -q -O- https://downloads.opennebula.org/repo/repo.key | sudo apt-key add -
sudo apt-get update
sudo apt install opennebula opennebula-sunstone opennebula-gate opennebula-flow
sudo /usr/share/one/install_gems

* Use UN as 'oneadmin' & below PW for login
sudo cat ~/.one/one_auth

sudo -i
su oneadmin
systemctl start opennebula opennebula-sunstone
oneuser show

* Type below url in browser.
http://localhost:9869

---------------------
* KVM-node installation (vm2)

sudo -i
wget -q -O- https://downloads.opennebula.org/repo/repo.key | sudo apt-key add -
sudo apt-get update
sudo apt-get install opennebula-node
sudo service libvirt-bin restart

--------------------------------------------------------------
# Ex 8, 9: Create VM components & Live migration

sudo -i
su oneadmin

onehost list
onevnet list
oneimage list
onetemplate list

* Get SSH key to paste it in the user settings.
cat /var/lib/one/.ssh/id_rsa.pub

onevm list

onevm deploy 3 3 (vm_id -> host_id)
onevm migrate 3 4 (host1_id -> host2_id)

--------------------------------------------------------------
# Ex 10: hadoop

sudo apt-get install default-jdk

sudo adduser hduser
sudo usermod -G sudo hduser
su hduser

sudo apt-get install openssh-server
ssh-keygen -t rsa -p '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/id_rsa
ssh localhost

sudo tar xfz hadoop_tar_file
sudo mv hadoop_2.7.7 /usr/local/hadoop

sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh
* Identify the below line & set below path
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"

sudo nano ~/.bashrc
export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=$JAVA_HOME/lib

#HADOOP VARIABLES START
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native export HADOOP_OPTS=“-Djava.library.path=$HADOOP_HOME/lib”
export HADOOP_CLASSPATH=$JAVA_HOME/lib/tools.jar
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
#HADOOP VARIABLES END

source ~/.bashrc

sudo nano /usr/local/hadoop/etc/hadoop/core-site.xml
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>

sudo nano /usr/local/hadoop/etc/hadoop/yarn-site.xml
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.Shufflehandler</value>
</property>

* If mapred-site.xml file not found,
sudo cp mapred-site.xml.template mapred-site.xml
sudo nano /usr/local/hadoop/etc/hadoop/mapred-site.xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</yarn>
<property>

sudo mkdir -p /usr/local/hadoop_store/hdfs

sudo nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml
<property>
<name>dfs.replication</name>
<value>1</value>
<property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/namenode</value>
<property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/datanode</value>
<property>

sudo chown hduser:hduser -R /usr/local/hadoop
sudo chown hduser:hduser -R /usr/local/hadoop_store
sudo chmod -R 777 /usr/local/hadoop
sudo chmod -R 777 /usr/local/hadoop_store

/usr/local/hadoop/bin/hadoop namenode -format
bash /usr/local/hadoop/sbin/start-all.sh
jps

* Type below url in browser.
http://localhost:8088

* Type some content in newly created file.
sudo nano input.txt

hdfs dfs -mkdir -p /user/hadoop/inputfiles
hdfs dfs -put input.txt /user/hadoop/inputfiles
hdfs dfs -ls /user/hadoop/inputfiles

---------------------
* WordCount.java

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}

---------------------

/usr/local/hadoop/bin/hadoop com.sun.tools.javac.Main WordCount.java
jar cf wc.jar WordCount*.class
ls
/usr/local/hadoop/bin/hadoop jar wc.jar WordCount /user/hadoop/inputfiles /user/hadoop/outputfiles
hdfs dfs -ls /user/hadoop/outputfiles
hdfs dfs -cat /user/hadoop/outputfiles/part-r-00000

--------------------------------------------------------------
# Ex 11: Hadoop fuse

wget https://archive.cloudera.com/cdh5/one-click-install/trusty/amd64/cdh5-repository_1.0_all.deb
sudo dpkg -i cdh5-repository_1.0_all.deb
sudo apt-get update 
sudo apt-get install hadoop-hdfs-fuse
sudo mkdir -p /mnt/hdfs
hadoop-fuse-dfs dfs://localhost:50070 /mnt/hdfs

--------------------------------------------------------------
# Helpline (If any error occurs in above steps, refer below)

# Ex 3-5

sudo add-apt-repository ppa:webupd8team/java
sudo apt-get install openjdk-8-jre
sudo apt-get install openjdk-8-jdk
sudo apt-get install openssh-server openssh-client

---------------------
# Ex 6

chmod 0700 ~/.euca
sudo euca_conf --get-credentials file.zip

---------------------
# To resolve 'Could not get lock' error

sudo rm /var/lib/apt/lists/lock
sudo rm /var/cache/apt/archives/lock
sudo rm /var/lib/dpkg/lock

--------------------------------------------------------------
